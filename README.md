# Parallel Computing Projects
Parallel Computing Projects: Explore projects focusing on leveraging multiple processors or computers to perform computations simultaneously, improving efficiency and scalability in various applications.

## 1. N-Body Simulation via MPI

This project involves simulating N-body systems using the Message Passing Interface (MPI) for parallelization. MPI allows for distributed memory parallelism, enabling efficient computation across multiple processors or nodes.

## 2. N-Body Simulation via OpenMP

Similar to the MPI implementation, this project focuses on N-body simulation but utilizes OpenMP for parallelization. OpenMP is a shared memory parallelization API, often used for multithreading within a single compute node.

## 3. Message-passing parallelism

This project explores the concept of message-passing parallelism, a fundamental approach in parallel computing. It involves exchanging data between parallel processes or threads via message passing, commonly used in distributed memory systems.

## 4. Shared memory parallelism

Contrasting with message-passing parallelism, this project delves into shared memory parallelism. It involves parallelizing computations by allowing multiple threads to access shared memory within a single compute node.

## 5. A Simple CUDA Renderer

This project focuses on building a simple renderer using CUDA, NVIDIA's parallel computing platform and programming model. CUDA enables developers to harness the power of NVIDIA GPUs for parallel computation tasks, such as rendering graphics.

## 6. Building A Task Execution Library from the Ground Up

This project involves constructing a task execution library, likely aimed at facilitating parallel task execution across distributed or shared memory systems. Building such libraries often requires understanding parallel computing principles and efficient task scheduling algorithms.

## 7. Chat149 - A Flash Attention Transformer DNN

Chat149 is a project centered around implementing a Flash Attention Transformer DNN (Deep Neural Network). This likely involves parallelizing computations within the transformer architecture, a popular model for various natural language processing tasks like language translation and text generation.

These projects showcase different parallel computing paradigms and applications, ranging from simulations to deep learning models and parallel libraries development. Each project offers valuable insights into parallel programming techniques and their practical implementations.
